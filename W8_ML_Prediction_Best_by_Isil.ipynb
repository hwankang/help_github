{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W8 ML Prediction Best by Isil.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwankang/help_github/blob/main/W8_ML_Prediction_Best_by_Isil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW7jkW7wXmF9"
      },
      "source": [
        "## ML 마스터노트 실무예제\n",
        "* Isil Berkun, Intel Data Scientist\n",
        "* Prediction on the multi-attribute dataset\n",
        "* Lecture: Junsang Dong (naebon1@gmail.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3TOC3_upgz5"
      },
      "source": [
        "### 미션1. 아래 소스코드를 완성하고 5개의 머신러닝 모델의 예측 성능을 확인하세요!\n",
        "\n",
        "### 미션2. UCI 데이터저장소에서 적절한 데이터를 선택한 후 이번 실무예제 소스코드에 적용해서 데이터를 분석하세요!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqXyeJ2MTj-x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1bdc2384-af1f-4c18-c37c-19116cd04ec7"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Apr 28 15:46:31 2019\n",
        "\n",
        "@author: berkunis\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nCreated on Sun Apr 28 15:46:31 2019\\n\\n@author: berkunis\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obXkrQGJX55j"
      },
      "source": [
        "### 1. 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH_uulNETnhM"
      },
      "source": [
        "import numpy as np\n",
        "import ### as pd\n",
        "import ###.pyplot as plt \n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.### import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import ###\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import ###\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.### import RandomForestRegressor\n",
        "\n",
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ4yH716YrPs"
      },
      "source": [
        "### 2. 데이터세트 로딩 (CSV 파일 읽기)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyCf0upBTtPx"
      },
      "source": [
        "# 데이터 임포트1. 런타임\n",
        "# import data\n",
        "# insurance.csv\n",
        "data = ###\n",
        "\n",
        "#see the first 15 lines of data\n",
        "print(data.head(15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vld7yMHOTwKA"
      },
      "source": [
        "# 데이터 임포트2. 구글 드라이브\n",
        "# 구글 드라이브에 dataset 폴더를 만들고 대상 파일 업로드\n",
        "# mount()\n",
        "\n",
        "from google.colab import ###\n",
        "drive.###('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfWwDlPuW0Gb"
      },
      "source": [
        "data = pd.read_csv(\"#구글 드라이브 경로지정#\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XUc_pCDY9sI"
      },
      "source": [
        "data.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeveGUmiZC_5"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ohC5cZ7ZUR-"
      },
      "source": [
        "str(len(data))+'개의 인스턴스 존재'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNCus2K0e2z1"
      },
      "source": [
        "### 3. 누락값 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fRyl4RyUlVI"
      },
      "source": [
        "#check how many values are missing (NaN) before we apply the methods below \n",
        "#isnull()\n",
        "#count_nan > 0\n",
        "\n",
        "count_nan = data.###().sum() # the number of missing values for every column\n",
        "print(count_nan[###])\n",
        "\n",
        "#fill in the missing values (we will look at 4 options for this course - there are so many other methods out there.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybylpuqzUpc0"
      },
      "source": [
        "#option0 for dropping the entire column\n",
        "#drop()\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/My Drive/dataset/insurance.csv\") # reloading fresh dataset for option 0\n",
        "data.###('bmi', axis = 1, inplace = True)\n",
        "print(data.head(15))\n",
        "\n",
        "#check how many values are missing (NaN) - after we dropped 'bmi'\n",
        "count_nan = data.isnull().sum() # the number of missing values for every column\n",
        "print(count_nan[count_nan > 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gCcURWBUp1R"
      },
      "source": [
        "#option1 for dropping NAN\n",
        "#dropna()\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/My Drive/dataset/insurance.csv\") # reloading fresh dataset for option 1\n",
        "data.###(inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "print(data.head(15))\n",
        "\n",
        "#check how many values are missing (NaN) - after we filled in the NaN\n",
        "count_nan = data.isnull().sum() # the number of missing values for every column\n",
        "print(count_nan[count_nan > 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NrY1Q-tUpqi"
      },
      "source": [
        "#option2 for filling NaN # reloading fresh dataset for option 2\n",
        "#SimpleImputer()\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/My Drive/dataset/insurance.csv\")\n",
        "imputer = ###(strategy='mean')\n",
        "imputer.fit(data['bmi'].values.reshape(-1, 1))\n",
        "data['bmi'] = imputer.transform(data['bmi'].values.reshape(-1, 1))\n",
        "print(data.head(15))\n",
        "\n",
        "#check how many values are missing (NaN) - after we filled in the NaN\n",
        "count_nan = data.isnull().sum() # the number of missing values for every column\n",
        "print(count_nan[count_nan > 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2zO6vTWUBAo"
      },
      "source": [
        "#option3 for filling NaN # reloading fresh dataset for option 3\n",
        "#fillna()\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/My Drive/dataset/insurance.csv\")\n",
        "data['bmi'].###(data['bmi'].mean(), inplace = True)\n",
        "print(data.head(15))\n",
        "\n",
        "#check how many values are missing (NaN) - after we filled in the NaN\n",
        "count_nan = data.isnull().sum() # the number of missing values for every column\n",
        "print(count_nan[count_nan > 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KszJ8akfMoY"
      },
      "source": [
        "### 4. 데이터 변환 (카테고리형 데이터를 숫자형 데이터로 변환)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JoDASgqUy9V"
      },
      "source": [
        "#option0: pandas factorizing: maps each category to a different integer = label encoder \n",
        "\n",
        "#create series for pandas\n",
        "#factorize()\n",
        "\n",
        "region = data[\"region\"] # series \n",
        "region_encoded, region_categories = pd.###(region)\n",
        "factor_region_mapping = dict(zip(region_categories, region_encoded)) #mapping of encoded numbers and original categories. \n",
        "\n",
        "print(\"Pandas factorize function for label encoding with series\")  \n",
        "print(region[:10]) #original version \n",
        "print(region_categories) #list of categories\n",
        "print(region_encoded[:10]) #encoded numbers for categories \n",
        "print(factor_region_mapping) # print factor mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRJuuQpmU4qv"
      },
      "source": [
        "#option1: pandas get_dummies: maps each category to 0 (cold) or 1 (hot) = one hot encoder \n",
        "\n",
        "#create series for pandas\n",
        "#get_dummies()\n",
        "\n",
        "region = data[\"region\"] # series \n",
        "region_encoded = pd.###(region, prefix='')\n",
        "\n",
        "print(\"Pandas get_dummies function for one hot encoding with series\")  \n",
        "\n",
        "print(region[:10]) #original version \n",
        "print(region_encoded[:10]) #encoded numbers for categories "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KXP9I6hU7PZ"
      },
      "source": [
        "#option2: sklearn label encoding: maps each category to a different integer\n",
        "\n",
        "#create ndarray for label encodoing (sklearn)\n",
        "sex = data.iloc[:,1:2].values\n",
        "smoker = data.iloc[:,4:5].values\n",
        "\n",
        "#label encoder = le\n",
        "\n",
        "## le for sex\n",
        "#LabelEncoder()\n",
        "\n",
        "le = ###()\n",
        "sex[:,0] = le.fit_transform(sex[:,0])\n",
        "sex = pd.DataFrame(sex)\n",
        "sex.columns = ['sex']\n",
        "le_sex_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"Sklearn label encoder results for sex:\")  \n",
        "print(le_sex_mapping)\n",
        "print(sex[:10])\n",
        "\n",
        "## le for smoker\n",
        "le = ###()\n",
        "smoker[:,0] = le.fit_transform(smoker[:,0])\n",
        "smoker = pd.DataFrame(smoker)\n",
        "smoker.columns = ['smoker']\n",
        "le_smoker_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"Sklearn label encoder results for smoker:\")  \n",
        "print(le_smoker_mapping)\n",
        "print(smoker[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua6ykInLyOQl"
      },
      "source": [
        "#option3: sklearn one hot encoding: maps each category to 0 (cold) or 1 (hot) \n",
        "\n",
        "#one hot encoder = ohe\n",
        "\n",
        "#create ndarray for one hot encodoing (sklearn)\n",
        "region = data.iloc[:,5:6].values #ndarray\n",
        "\n",
        "## ohe for region\n",
        "#OneHotEncoder()\n",
        "\n",
        "ohe = ###() \n",
        "\n",
        "region = ohe.fit_transform(region).toarray()\n",
        "region = pd.DataFrame(region)\n",
        "region.columns = ['northeast', 'northwest', 'southeast', 'southwest']\n",
        "print(\"Sklearn one hot encoder results for region:\")  \n",
        "print(region[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khsDy76efeXA"
      },
      "source": [
        "### 5. 데이터세트 분리 (훈련 데이터와 검증 데이터로 분리)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsakXQYyUYtp"
      },
      "source": [
        "#putting the data together:\n",
        "\n",
        "##take the numerical data from the original data\n",
        "#copy()\n",
        "\n",
        "X_num = data[['age', 'bmi', 'children']].###()\n",
        "\n",
        "##take the encoded data and add to numerical data\n",
        "#concat()\n",
        "\n",
        "X_final = pd.###([X_num, region, sex, smoker], axis = 1)\n",
        "\n",
        "#define y as being the \"charges column\" from the original dataset\n",
        "y_final = data[['charges']].copy()\n",
        "\n",
        "#Test train split\n",
        "X_train, X_test, y_train, y_test = ###(X_final, y_final, test_size = 0.33, random_state = 0 )\n",
        "#X_train, X_test, y_train, y_test = train_test_split(data[['age']], y_final, test_size = 0.33, random_state = 0 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdZoYrI8ftZz"
      },
      "source": [
        "### 6. 특성치 정규화 (또는 표준화)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7XXI2uCfy4x"
      },
      "source": [
        "###normalized scaler (fit transform on train, fit only on test)\n",
        "#MinMaxScaler()\n",
        "\n",
        "#n_scaler = ###()\n",
        "#X_train = n_scaler.fit_transform(X_train.astype(np.float))\n",
        "#X_test= n_scaler.transform(X_test.astype(np.float))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnCZLdaJUWOj"
      },
      "source": [
        "#standard scaler (fit transform on train, fit only on test)\n",
        "#StandardScaler()\n",
        "\n",
        "s_scaler = ###()\n",
        "X_train = s_scaler.fit_transform(X_train.astype(np.float))\n",
        "X_test= s_scaler.transform(X_test.astype(np.float))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36jC5Y6sf4d6"
      },
      "source": [
        "### 7. ML 모델 기반 예측"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEoUnKPJgCmL"
      },
      "source": [
        "#### 1) 선형회귀모델 / LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anwEusk2URZ_"
      },
      "source": [
        "#LinearRegression()\n",
        "#fit()\n",
        "#predict()\n",
        "\n",
        "lr = ###().###(X_train,y_train)\n",
        "y_train_pred = lr.###(X_train)\n",
        "y_test_pred = lr.###(X_test)\n",
        "\n",
        "#print score\n",
        "print(\"lr.coef_: {}\".format(lr.coef_))\n",
        "print(\"lr.intercept_: {}\".format(lr.intercept_))\n",
        "print('lr train score %.3f, lr test score: %.3f' % (\n",
        "lr.score(X_train,y_train),\n",
        "lr.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4o2ClXBgPGL"
      },
      "source": [
        "#### 2) 다항회귀모델 / PolynomialRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q85LKH7YUPhB"
      },
      "source": [
        "#PolynomialFeatures()\n",
        "#fit_transform()\n",
        "\n",
        "poly = ###(degree = 3)\n",
        "X_poly = poly.###(X_final)\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X_poly,y_final, test_size = 0.33, random_state = 0)\n",
        "\n",
        "#standard scaler (fit transform on train, fit only on test)\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train.astype(np.float))\n",
        "X_test= sc.transform(X_test.astype(np.float))\n",
        "\n",
        "#fit model\n",
        "poly_lr = LinearRegression().###(X_train,y_train)\n",
        "y_train_pred = poly_lr.predict(X_train)\n",
        "y_test_pred = poly_lr.predict(X_test)\n",
        "\n",
        "#print score\n",
        "print('poly train score %.3f, poly test score: %.3f' % (\n",
        "poly_lr.score(X_train,y_train),\n",
        "poly_lr.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw3zGClQgaRl"
      },
      "source": [
        "#### 3) SVM 회귀모델 / SVR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYcokNrIUNK5"
      },
      "source": [
        "#SVR()\n",
        "#StandardScaler()\n",
        "\n",
        "svr = ###(kernel='linear', C = 300)\n",
        "\n",
        "#test train split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.33, random_state = 0 )\n",
        "\n",
        "#standard scaler (fit transform on train, fit only on test)\n",
        "sc = ###()\n",
        "X_train = sc.fit_transform(X_train.astype(np.float))\n",
        "X_test= sc.transform(X_test.astype(np.float))\n",
        "\n",
        "#fit model\n",
        "svr = svr.###(X_train,y_train.values.ravel())\n",
        "y_train_pred = svr.predict(X_train)\n",
        "y_test_pred = svr.predict(X_test)\n",
        "\n",
        "#print score\n",
        "print('svr train score %.3f, svr test score: %.3f' % (\n",
        "svr.score(X_train,y_train),\n",
        "svr.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaG_rhmngkIh"
      },
      "source": [
        "#### 4) 의사결정수 회귀모델 / DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxJhZyGsULmh"
      },
      "source": [
        "#DecisionTreeRegressor()\n",
        "#StandardScaler()\n",
        "\n",
        "dt = ###(random_state=0)\n",
        "\n",
        "#test train split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.33, random_state = 0 )\n",
        "\n",
        "#standard scaler (fit transform on train, fit only on test)\n",
        "sc = ###()\n",
        "X_train = sc.fit_transform(X_train.astype(np.float))\n",
        "X_test= sc.transform(X_test.astype(np.float))\n",
        "\n",
        "\n",
        "#fit model\n",
        "dt = dt.fit(X_train,###.values.ravel())\n",
        "y_train_pred = dt.predict(X_train)\n",
        "y_test_pred = dt.predict(###)\n",
        "\n",
        "#print score\n",
        "print('dt train score %.3f, dt test score: %.3f' % (\n",
        "dt.score(X_train,y_train),\n",
        "dt.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-FgZWD2guox"
      },
      "source": [
        "#### 5) 랜덤포레스트 회귀모델 / RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCGKYS84UH18"
      },
      "source": [
        "#RandomForestRegressor()\n",
        "#StandardScaler()\n",
        "\n",
        "forest = ###(n_estimators = 100,\n",
        "                              criterion = 'mse',\n",
        "                              random_state = 1,\n",
        "                              n_jobs = -1)\n",
        "#test train split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.33, random_state = 0 )\n",
        "\n",
        "#standard scaler (fit transform on train, fit only on test)\n",
        "sc = ###()\n",
        "X_train = sc.fit_transform(X_train.astype(np.float))\n",
        "X_test= sc.transform(X_test.astype(np.float))\n",
        "\n",
        "#fit model\n",
        "forest.fit(###,y_train.values.ravel())\n",
        "y_train_pred = forest.predict(X_train)\n",
        "y_test_pred = forest.predict(X_test)\n",
        "\n",
        "#print score\n",
        "print('forest train score %.3f, forest test score: %.3f' % (\n",
        "forest.score(X_train,y_train),\n",
        "forest.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMu4kr6LXZaW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}